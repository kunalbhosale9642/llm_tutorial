# llm_tutorial
Using local LLM model creating a chatting tool

#step 1    
#install ollama go to link --> https://ollama.com/download

#step 2   
#Take pull of LLM models (minimum 8gb ram required)    
ollama pull gemma3
ollama pull nomic-embed-text

#step 3    
#Run LLM locally from terminal    
ollama serve

#step 4    
#install python

#step 5   
pip install -r requirements.txt



