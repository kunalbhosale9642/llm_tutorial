# llm_tutorial
VaultQuery: Private AI for Your PDFs

#step 1    
#install ollama go to link --> https://ollama.com/download

#step 2   
#Take pull of LLM models (minimum 8gb ram required)    
ollama pull gemma3
ollama pull nomic-embed-text

#step 3    
#Run LLM locally from terminal    
ollama serve

#step 4    
#install python

#step 5   
pip install -r requirements.txt

#step 6
run command in terminal -> python .\pdf_api.py

#step 7
hit live server from vs code and then on browser type -> http://127.0.0.1:5500/




